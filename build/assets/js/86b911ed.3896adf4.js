"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[887],{4657:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var i=n(4848),t=n(8453);const r={},a="SAM2 on Images",o={id:"SAM2 on Images",title:"SAM2 on Images",description:"Issues faces on Grayscale images",source:"@site/docs/SAM2 on Images.mdx",sourceDirName:".",slug:"/SAM2 on Images",permalink:"/docs/SAM2 on Images",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Pipeline",permalink:"/docs/Pipeline"},next:{title:"Usable Result",permalink:"/docs/Usable Result"}},l={},d=[{value:"Issues faces on Grayscale images",id:"issues-faces-on-grayscale-images",level:2},{value:"Other Examples:",id:"other-examples",level:2},{value:"Even after tweaking the optimal parameters, the ambiguous region as mentioned remains unmasked.",id:"even-after-tweaking-the-optimal-parameters-the-ambiguous-region-as-mentioned-remains-unmasked",level:4},{value:"Proper colour segmentation is needed otherwise same object gets two different masks",id:"proper-colour-segmentation-is-needed-otherwise-same-object-gets-two-different-masks",level:4},{value:"Finding Optimal Parameters",id:"finding-optimal-parameters",level:3},{value:"Example Illustrating the Limitations of AutoMask and the Enhanced Capabilities of Tweaked Mask",id:"example-illustrating-the-limitations-of-automask-and-the-enhanced-capabilities-of-tweaked-mask",level:4},{value:"Understanding what each of the parameters does:",id:"understanding-what-each-of-the-parameters-does",level:3},{value:"SAM2AutomaticMaskGenerator returns a list of masks, where each mask is a dict containing various information about the mask",id:"sam2automaticmaskgenerator-returns-a-list-of-masks-where-each-mask-is-a-dict-containing-various-information-about-the-mask",level:2},{value:"Understanding the Impact of the <code>crop_n_layers</code> Parameter",id:"understanding-the-impact-of-the-crop_n_layers-parameter",level:3},{value:"Observation Based on <code>crop_n_layers</code>:",id:"observation-based-on-crop_n_layers",level:4},{value:"<code>points_per_side</code> Parameter:",id:"points_per_side-parameter",level:3},{value:"Impact on Segmentation:",id:"impact-on-segmentation",level:4}];function c(e){const s={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"sam2-on-images",children:"SAM2 on Images"})}),"\n",(0,i.jsx)(s.h2,{id:"issues-faces-on-grayscale-images",children:"Issues faces on Grayscale images"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"alt text",src:n(4864).A+"",width:"1436",height:"648"})}),"\n",(0,i.jsx)(s.p,{children:"Lack of Proper colour or boundary Separation in images confuses the mask and causes either of the two:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:"No masks around the ambiguous region."}),"\n",(0,i.jsx)(s.li,{children:"Insufficient mask or improper segmentation."}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"other-examples",children:"Other Examples:"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"alt text",src:n(5879).A+"",width:"3372",height:"1124"})}),"\n",(0,i.jsx)(s.h4,{id:"even-after-tweaking-the-optimal-parameters-the-ambiguous-region-as-mentioned-remains-unmasked",children:"Even after tweaking the optimal parameters, the ambiguous region as mentioned remains unmasked."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"alt text",src:n(3942).A+"",width:"2512",height:"840"})}),"\n",(0,i.jsx)(s.h4,{id:"proper-colour-segmentation-is-needed-otherwise-same-object-gets-two-different-masks",children:"Proper colour segmentation is needed otherwise same object gets two different masks"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"alt text",src:n(4661).A+"",width:"3320",height:"1122"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)(s.h3,{id:"finding-optimal-parameters",children:"Finding Optimal Parameters"}),"\n",(0,i.jsx)(s.p,{children:"By default SAM2 Uses the AutoMaskGenerator function to generate masks with uses all default parameters for the mask.\nThe deafault parameters are specified below:"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:'class SAM2AutomaticMaskGenerator:\n    def __init__(\n        self,\n        model: SAM2Base,\n        points_per_side: Optional[int] = 32,\n        points_per_batch: int = 64,\n        pred_iou_thresh: float = 0.8,\n        stability_score_thresh: float = 0.95,\n        stability_score_offset: float = 1.0,\n        mask_threshold: float = 0.0,\n        box_nms_thresh: float = 0.7,\n        crop_n_layers: int = 0,\n        crop_nms_thresh: float = 0.7,\n        crop_overlap_ratio: float = 512 / 1500,\n        crop_n_points_downscale_factor: int = 1,\n        point_grids: Optional[List[np.ndarray]] = None,\n        min_mask_region_area: int = 0,\n        output_mode: str = "binary_mask",\n        use_m2m: bool = False,\n        multimask_output: bool = True,\n    ) -> None:\n'})}),"\n",(0,i.jsx)(s.h4,{id:"example-illustrating-the-limitations-of-automask-and-the-enhanced-capabilities-of-tweaked-mask",children:"Example Illustrating the Limitations of AutoMask and the Enhanced Capabilities of Tweaked Mask"}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{children:"Source Image"}),(0,i.jsx)(s.th,{children:"Auto Mask Segmentation"}),(0,i.jsx)(s.th,{children:"Tweaked Parameter Mask"})]})}),(0,i.jsx)(s.tbody,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.img,{alt:"alt text",src:n(1878).A+"",width:"500",height:"806"})}),(0,i.jsx)(s.td,{children:(0,i.jsx)(s.img,{alt:"alt text",src:n(3645).A+"",width:"492",height:"802"})}),(0,i.jsx)(s.td,{children:(0,i.jsx)(s.img,{alt:"alt text",src:n(404).A+"",width:"498",height:"892"})})]})})]}),"\n",(0,i.jsx)(s.h3,{id:"understanding-what-each-of-the-parameters-does",children:"Understanding what each of the parameters does:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"model"}),": ",(0,i.jsx)(s.code,{children:"SAM2Base"}),(0,i.jsx)(s.br,{}),"\n","The SAM2 model used for mask prediction. It should be an instance of the ",(0,i.jsx)(s.code,{children:"SAM2Base"})," class or its derivatives."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"points_per_side"}),": ",(0,i.jsx)(s.code,{children:"Optional[int]"}),(0,i.jsx)(s.br,{}),"\n","Specifies the number of points to sample along one side of the image. The total number of points will be ",(0,i.jsx)(s.code,{children:"points_per_side ** 2"}),". If this parameter is ",(0,i.jsx)(s.code,{children:"None"}),", the ",(0,i.jsx)(s.code,{children:"point_grids"})," parameter must provide explicit point sampling grids."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"points_per_batch"}),": ",(0,i.jsx)(s.code,{children:"int"}),(0,i.jsx)(s.br,{}),"\n","Determines the number of points to be processed simultaneously by the model. Increasing this number can improve processing speed but will also increase GPU memory usage."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"pred_iou_thresh"}),": ",(0,i.jsx)(s.code,{children:"float"}),(0,i.jsx)(s.br,{}),"\n","A filtering threshold in the range ",(0,i.jsx)(s.code,{children:"[0,1]"})," that uses the model's predicted mask quality (Intersection over Union, IoU) to filter out low-quality masks."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"stability_score_thresh"}),": ",(0,i.jsx)(s.code,{children:"float"}),(0,i.jsx)(s.br,{}),"\n","A filtering threshold in the range ",(0,i.jsx)(s.code,{children:"[0,1]"})," based on the stability of the mask when the cutoff used to binarize the model's mask predictions is changed."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"stability_score_offset"}),": ",(0,i.jsx)(s.code,{children:"float"}),(0,i.jsx)(s.br,{}),"\n","The amount by which to shift the cutoff when calculating the stability score. This helps in determining the robustness of the generated masks."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"mask_threshold"}),": ",(0,i.jsx)(s.code,{children:"float"}),(0,i.jsx)(s.br,{}),"\n","The threshold for binarizing the mask logits. This value determines the cutoff for classifying pixels as foreground or background in the segmentation mask."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"box_nms_thresh"}),": ",(0,i.jsx)(s.code,{children:"float"}),(0,i.jsx)(s.br,{}),"\n","The IoU threshold used in Non-Maximum Suppression (NMS) to filter out duplicate masks. Higher values will keep more masks, potentially increasing redundancy."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"crop_n_layers"}),": ",(0,i.jsx)(s.code,{children:"int"}),(0,i.jsx)(s.br,{}),"\n","If greater than 0, the mask prediction will be run iteratively on crops of the image. This parameter sets the number of cropping layers to use, where each layer has ",(0,i.jsx)(s.code,{children:"2**i_layer"})," number of image crops."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"crop_nms_thresh"}),": ",(0,i.jsx)(s.code,{children:"float"}),(0,i.jsx)(s.br,{}),"\n","Similar to ",(0,i.jsx)(s.code,{children:"box_nms_thresh"}),", this parameter sets the IoU threshold for NMS to filter out duplicate masks between different crops."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"crop_overlap_ratio"}),": ",(0,i.jsx)(s.code,{children:"float"}),(0,i.jsx)(s.br,{}),"\n","Sets the degree of overlap between image crops. In the first crop layer, crops will overlap by this fraction of the image length. For subsequent layers with more crops, the overlap is scaled down accordingly."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"crop_n_points_downscale_factor"}),": ",(0,i.jsx)(s.code,{children:"int"}),(0,i.jsx)(s.br,{}),"\n","Controls the downscaling of points-per-side sampled in each layer. The number of points per side in layer ",(0,i.jsx)(s.code,{children:"n"})," is reduced by a factor of ",(0,i.jsx)(s.code,{children:"crop_n_points_downscale_factor ** n"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"point_grids"}),": ",(0,i.jsx)(s.code,{children:"Optional[List[np.ndarray]]"}),(0,i.jsx)(s.br,{}),"\n","A list of explicit grids of points used for sampling, normalized to the ",(0,i.jsx)(s.code,{children:"[0,1]"})," range. The nth grid in the list is used in the nth crop layer. This parameter is mutually exclusive with ",(0,i.jsx)(s.code,{children:"points_per_side"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"min_mask_region_area"}),": ",(0,i.jsx)(s.code,{children:"int"}),(0,i.jsx)(s.br,{}),"\n","If greater than 0, post-processing will be applied to remove small, disconnected regions and holes in masks that are smaller than the specified area. This requires OpenCV."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"output_mode"}),": ",(0,i.jsx)(s.code,{children:"str"}),(0,i.jsx)(s.br,{}),"\n","Specifies the format in which masks are returned. Can be one of the following:"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"binary_mask"}),": Returns masks as binary arrays."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"uncompressed_rle"}),": Returns masks in uncompressed Run-Length Encoding (RLE) format."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"coco_rle"}),": Returns masks in COCO-style RLE format, requiring pycocotools."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"use_m2m"}),": ",(0,i.jsx)(s.code,{children:"bool"}),(0,i.jsx)(s.br,{}),"\n","Determines whether to use a one-step refinement process that utilizes previous mask predictions for further refining the output."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"multimask_output"}),": ",(0,i.jsx)(s.code,{children:"bool"}),(0,i.jsx)(s.br,{}),"\n","Indicates whether to output multiple masks for each point in the grid. This can be useful for generating different mask hypotheses at each point."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"kwargs"}),(0,i.jsx)(s.br,{}),"\n","Additional keyword arguments that may be used for further customization."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"sam2automaticmaskgenerator-returns-a-list-of-masks-where-each-mask-is-a-dict-containing-various-information-about-the-mask",children:"SAM2AutomaticMaskGenerator returns a list of masks, where each mask is a dict containing various information about the mask"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"segmentation - [np.ndarray] - the mask with (W, H) shape, and bool type\n"})}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"area - [int] - the area of the mask in pixels\n"})}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"bbox - [List[int]] - the boundary box of the mask in xywh format\n"})}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"predicted_iou - [float] - the model's own prediction for the quality of the mask\n"})}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"point_coords - [List[List[float]]] - the sampled input point that generated this mask\n"})}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"stability_score - [float] - an additional measure of mask quality\n"})}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"crop_box - List[int] - the crop of the image used to generate this mask in xywh format\n"})}),"\n",(0,i.jsxs)(s.h3,{id:"understanding-the-impact-of-the-crop_n_layers-parameter",children:["Understanding the Impact of the ",(0,i.jsx)(s.code,{children:"crop_n_layers"})," Parameter"]}),"\n",(0,i.jsxs)(s.p,{children:["The ",(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"crop_n_layers"})})," parameter controls the number of iterative cropping layers applied during segmentation. Each layer increases the number of crops, allowing for a more detailed analysis of the image, focusing on different areas and refining the segmentation with each layer."]}),"\n",(0,i.jsxs)(s.h4,{id:"observation-based-on-crop_n_layers",children:["Observation Based on ",(0,i.jsx)(s.code,{children:"crop_n_layers"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Source Image"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"The original image shows a busy traffic scene with numerous vehicles and motorcyclists. It\u2019s complex, with many overlapping and closely packed objects."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"n=0 (Crop Layer 0)"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["At the initial crop layer (",(0,i.jsx)(s.code,{children:"n=0"}),"), the segmentation is coarse, similar to the basic segmentation without any crops. The objects such as vehicles and people are roughly identified, but the boundaries are not very precise. Overlapping or closely situated objects may not be distinctly separated, and there is minimal refinement."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"n=1 (Crop Layer 1)"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["With the first cropping layer (",(0,i.jsx)(s.code,{children:"n=1"}),"), the segmentation begins to refine. More objects are identified, and the segmentation boundaries become more accurate compared to ",(0,i.jsx)(s.code,{children:"n=0"}),". The process of cropping allows the model to focus on smaller sections of the image, improving its ability to distinguish between closely situated objects and enhancing overall segmentation precision."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"n=2 (Crop Layer 2)"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Further refinement occurs at the second cropping layer (",(0,i.jsx)(s.code,{children:"n=2"}),"). The segmentation shows even more precise boundaries, and additional objects that were previously not segmented may now be identified. The masks conform more accurately to the shapes of individual vehicles and motorcyclists, indicating a higher resolution of segmentation due to increased focus from additional crops."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"n=3 (Crop Layer 3)"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["At the third cropping layer (",(0,i.jsx)(s.code,{children:"n=3"}),"), the segmentation reaches its most detailed level. The boundaries are very precise, and nearly every object in the image is segmented. The high number of crops allows the model to meticulously analyze and separate overlapping or closely packed objects, leading to a high level of accuracy and detail in object identification."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["The ",(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"crop_n_layers"})})," parameter significantly enhances the model's ability to perform detailed and accurate segmentation in complex scenes."]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["Lower ",(0,i.jsx)(s.code,{children:"crop_n_layers"})," Values"]})," (",(0,i.jsx)(s.code,{children:"n=0"})," or ",(0,i.jsx)(s.code,{children:"n=1"}),"):"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Results in less detailed segmentation, with coarser boundaries and less precise identification of objects. This is suitable for scenarios where broad object detection is sufficient."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["Higher ",(0,i.jsx)(s.code,{children:"crop_n_layers"})," Values"]})," (",(0,i.jsx)(s.code,{children:"n=2"})," or ",(0,i.jsx)(s.code,{children:"n=3"}),"):"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Leads to more detailed and refined segmentation, allowing the model to identify and separate closely packed or overlapping objects with high precision. This is ideal for complex scenes where fine details and accurate boundaries are crucial."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["In summary, increasing the ",(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"crop_n_layers"})})," enhances the model's segmentation detail and accuracy by allowing iterative analysis at different levels of image granularity, effectively focusing on smaller sections of the image and refining the segmentation process with each additional layer."]}),"\n",(0,i.jsxs)(s.h3,{id:"points_per_side-parameter",children:[(0,i.jsx)(s.code,{children:"points_per_side"})," Parameter:"]}),"\n",(0,i.jsxs)(s.p,{children:["The ",(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"points_per_side"})})," parameter determines how densely the points are sampled across the image for segmentation. The total number of points sampled is ",(0,i.jsx)(s.code,{children:"points_per_side ** 2"}),", meaning that increasing this parameter results in more points being considered for mask generation, while decreasing it results in fewer points."]}),"\n",(0,i.jsx)(s.h4,{id:"impact-on-segmentation",children:"Impact on Segmentation:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["Higher ",(0,i.jsx)(s.code,{children:"points_per_side"})," Value"]}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Finer Segmentation"}),": When ",(0,i.jsx)(s.code,{children:"points_per_side"})," is high, more points are sampled across the image. This means the model can capture finer details and more nuanced structures within the image. For example, if a car is considered a parent object and door handles, windows, and mirrors are considered child objects, a higher ",(0,i.jsx)(s.code,{children:"points_per_side"})," value can ensure that all these elements are included within the same mask. This is useful when you want to keep related parts grouped together as a single entity."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Less Separation"}),": The increased density of points helps in maintaining the relationship between the parent and child entities, such as ensuring that the door handle and the car door are not masked separately but as parts of the same object."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["Lower ",(0,i.jsx)(s.code,{children:"points_per_side"})," Value"]}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Coarser Segmentation"}),": With a lower ",(0,i.jsx)(s.code,{children:"points_per_side"})," value, fewer points are sampled. This leads to a coarser segmentation, where smaller or closely situated objects may not be accurately separated. The model might create separate masks for each part due to fewer points defining the object's boundary, resulting in entities like the car and its door handle being masked separately."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"More Separation"}),": Lower values make the model more likely to treat small details as distinct entities, resulting in separate masks for each object. This can be useful when the goal is to differentiate between closely situated or overlapping objects."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["In essence, the ",(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"points_per_side"})})," parameter controls the granularity of the segmentation process, affecting how the model distinguishes between parent and child entities or between different parts of an object. Adjusting this parameter allows for a flexible approach depending on the desired level of detail in segmentation."]})]})}function h(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},5879:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/ambi1-9b5c0f03be4c0f5a2c53a1629c0cc554.png"},4661:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/ambi3-ff2bfa625b17c2dd03ff6746a3981cc5.png"},3942:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/anbi2-24066700115f967272d28101e8bf6690.png"},4864:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/grayscale-f931bdae2c3330a2eb43d0453493f75a.png"},1878:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/image-1-4ff926f8ada3421f0383a0864b9bcce3.png"},3645:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/image-2-847477e97fc3fe9b524966532cf06fbd.png"},404:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/image-3-751658390b33a02ef2b61d2dac6a4d34.png"},8453:(e,s,n)=>{n.d(s,{R:()=>a,x:()=>o});var i=n(6540);const t={},r=i.createContext(t);function a(e){const s=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:s},e.children)}}}]);